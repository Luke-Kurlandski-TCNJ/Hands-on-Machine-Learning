{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "Linear Regression involves training a set of data with a bias term and one hyperparameter per feature.\n",
    "<br> A Linear Regression model's accuracy is measured with the MSE or RMSE cost function.\n",
    "### The Normal Equation\n",
    "Normal Equation - equation that minimizes the cost function\n",
    "<br>Scikit Learn has LinearRegression class that can compute linear regression easily.\n",
    "### Computational Complexity\n",
    "The LinearRegression class is O(n^2), while the Normal Equation is ~ O(n^2.75)\n",
    "### Gradient Descent\n",
    "Gradient Descent - generic optimization optimization algorithm\n",
    "<br> Gradient Descent operates by a testing a linear model, then modifying parameters so as to decrease the cost function.\n",
    "<br> Computes the partial derivative of a certain hyperparameter for each step.\n",
    "<br> Learning Rate - hyperparamter that describes how much each hyperparamter of the linear model will be changed per step in gradient descent\n",
    "<br> A Learning Rate that is too large will miss the minimum of cost function while a rate too small will take too long to compute.\n",
    "#### Batch Gradient Descent\n",
    "Batch Gradient Descent - use mathematical gradient of the cost function to figure out the direction to tweak hyperparamter\n",
    "<br> To find a decent learning rate, use grid search.\n",
    "<br> Batch Gradient Descent relies on the full training set, thus can be time consuming.\n",
    "#### Stochastic Gradient Descent\n",
    "Stochastic Gradient Descent - similar to batch, but selects a random instance to compute partial derivative and direction of hyperparameter tuning\n",
    "<br> Results in \"jumpy\" tuning, which can help to avoid local minima, but causes tuning to \"miss\" the global minima slightly.\n",
    "<br> Learning Schedule - function for the Learning Rate, which should reduce over time with Stochastic Gradient Descent algorithm\n",
    "#### Mini Batch Gradient Descent\n",
    "Mini Batch Gradient Descent - perform Batch Gradient Descent on a subset of the training set\n",
    "<br> This provides many of the benefits of both Batch Gradient Descent and Stochastic Gradient Descent.\n",
    "## Polynomial Regression\n",
    "Polynomial Regression - a more complex model useful for non-linear data\n",
    "<br> Beware of combinatoral explosion if choosing a high-order polynomial model.\n",
    "## Learning Curve\n",
    "If a model performs well on the training data, but poorly on validation set, then it is overfitting the data.\n",
    "<br> Learning Curves - plots of model's performance on training set and validation set (using cost function)\n",
    "### Bias/Variance Tradeoff\n",
    "Generalization Error can be expressed as the sum of three different kinds of errors:\n",
    "1. Bias - error caused by wrong assumptions, ex data is linear when in reality it is not\n",
    "2. Variance - error caused by excessive sensitivity to variations in data\n",
    "3. Irreducible - error caused by inherant noisyness of data\n",
    "\n",
    "Increasing the complexity of a model will increase its variance, but decrease its bias.\n",
    "## Regularized Linear Models\n",
    "Regularization of a model can reduce overfitting data.\n",
    "### Ridge Regression\n",
    "Ridge Regression - regularized version of Linear Regression that adds a regularization term to cost function related to the l2 norm\n",
    "<br> Effective with polynomial models.\n",
    "<br> Results in learning algorithm that tries to keep the constants in the model as in-extreme as possible and prevents overfitting.\n",
    "### Lasso Regression \n",
    "Least Absolute Shrinkage and Selection Operator Regression - similar to Ridge Regression, except uses l1 norm instead of l2.\n",
    "<br> Tends to eliminate weights of least important features. \n",
    "<br> Performs feature selection to output a sparse model.\n",
    "### Elastic Net\n",
    "Elastic Net - a combination of Ridge and Lasso Regression\n",
    "<br> User can select how potent each type of regression's influence should be.\n",
    "<br> Elastic Net can be used to implement 100% Ridge or 100% Lasso Regularization.\n",
    "### Early Stopping \n",
    "Early Stopping - regularization technique to stop training once optimial performance on validation set is achieved\n",
    "<br> Avoids overfitting data by simply reducing amount of data\n",
    "## Logistic Regression\n",
    "Logistic Regression - estimates the probability that an item is of a certain class\n",
    "### Estimating Probabilities\n",
    "Once the probablity is calculated, the algorithm can easily make the classification of yes or no.\n",
    "### Training and Cost Function \n",
    "Log Loss - logistic regression cost function\n",
    "<br> There is no known equation to compute the minimum of the Log Loss, so Gradient Descent method is used.\n",
    "### Decision Boundaries \n",
    "It is important to note that the logistic function could predict one class or another with extremely low confidence.\n",
    "### Softmax Regression\n",
    "Softmax Regression - generalized logistic regression model that can support multiple classes, without the need of numerous binary classifiers\n",
    "<br> Can only output one class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
